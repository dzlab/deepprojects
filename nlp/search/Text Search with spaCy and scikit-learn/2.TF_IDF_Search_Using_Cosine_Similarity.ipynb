{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.TF-IDF Search Using Cosine Similarity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juzFvWWxvwe7"
      },
      "source": [
        "# TF-IDF Search Using Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZQrdpUSSUNz"
      },
      "source": [
        "### 1. Imports\n",
        "Load all relevant Python libraries and a spaCy language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DXzf5Q56g6I"
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "import math\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyuyoSEYSLkD"
      },
      "source": [
        "### 2. Access tokens\n",
        "Access the tokenized text in your new dataset from the previous milestone. Each document dictionary should now include a new key-value pair with the lemmatized text of the articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-rvU-Nw6A11"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO25_Yk6R_Bo"
      },
      "source": [
        "### 3. Create a corpus vocabulary.\n",
        "It should simply be a list of unique tokens in the provided set of documents. Count how many times each unique token appears in the corpus, you will need these counts for the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ1WeR3H6O00"
      },
      "source": [
        "f = open('output.json',)\n",
        "data = json.load(f)\n",
        "corpus = []\n",
        "for obj in data:\n",
        "    tokens = obj['tokenized_text']\n",
        "    corpus = corpus + tokens"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XGZV35QRIOu"
      },
      "source": [
        "N = len(data)\n",
        "corpus_counts = Counter(corpus)\n",
        "corpus = list(corpus_counts.keys())"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II1ej9DrRkuK",
        "outputId": "ddf87012-c583-4f03-fd06-cb1a9c762e51"
      },
      "source": [
        "print('Number of documents: ', len(data))\n",
        "print('Number of unique tokens: ', len(corpus))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents:  26\n",
            "Number of unique tokens:  1999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFjaDm7fScm9"
      },
      "source": [
        "### 4. Calculate TF-IDF vectors\n",
        "Calculate Tf-Idf vectors for every article in the dataset and add these vectors to the article dictionaries. You should end up the same list of dictionaries as before, but with a new key-value pair containing Tf-Idf vectors: [link](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0yrmBu1SspP"
      },
      "source": [
        "def tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    return tokens\n",
        "\n",
        "def createVector(corpus, tokens):\n",
        "    # tf(t,d) = count of t in d / number of words in d\n",
        "    # df(t) = occurrence of t in documents\n",
        "    # idf(t) = log(N/(df + 1))\n",
        "    # tf-idf(t, d) = tf(t, d) * log(N/(df + 1))\n",
        "    doc_counts = Counter(tokens)\n",
        "    vector = []\n",
        "    for t in corpus_counts.keys():\n",
        "        tf = doc_counts[t] / len(tokens)\n",
        "        df = corpus_counts[t]\n",
        "        idf = math.log(N / (df + 1))\n",
        "        tf_idf = tf * idf\n",
        "        vector.append(tf_idf)\n",
        "    return vector"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRDL67syllEx"
      },
      "source": [
        "for d in data:\n",
        "    tokens = d['tokenized_text']\n",
        "    vector = createVector(corpus, tokens)\n",
        "    d['tf_idfs'] = vector"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjmXrPs5vpSJ"
      },
      "source": [
        "json_object = json.dumps(data, indent = 4)\n",
        "with open(\"output2.json\", \"w\") as outfile:\n",
        "    outfile.write(json_object)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPu8MTfPpEsp"
      },
      "source": [
        "### 5. Search using cosine_similarity\n",
        "Now we can try to search our list of dictionaries using this Tf-Idf field using existing tools for similarity. We suggest you use scikit-learn library and its cosine_similarity function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjrX-B7GuZLz"
      },
      "source": [
        "doc_vectors = [d['tf_idfs'] for d in data]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCbBClCApJKK"
      },
      "source": [
        "def search(doc_vectors, query_vector):\n",
        "    similarities = []\n",
        "    for doc_vector in doc_vectors:\n",
        "        x = np.array(doc_vector)\n",
        "        y = np.array(query_vector)\n",
        "        # Need to reshape these\n",
        "        x = x.reshape(1,-1)\n",
        "        y = y.reshape(1,-1)\n",
        "        similarities.append(cosine_similarity(x,y)[0][0])\n",
        "    return similarities"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICunpcNHpbgs",
        "outputId": "24db9278-9f03-4172-c2dc-aedf91d728b3"
      },
      "source": [
        "query = data[0]['text']\n",
        "query_tokens = tokenize(query)\n",
        "query_vector = createVector(corpus, query_tokens)\n",
        "search(doc_vectors, query_vector)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0000000000000004,\n",
              " 0.6577252805096234,\n",
              " 0.5570125628543748,\n",
              " 0.524118991779826,\n",
              " 0.500633081698554,\n",
              " 0.6155685311968865,\n",
              " 0.6300714676318564,\n",
              " 0.535137372425514,\n",
              " 0.5001747355065149,\n",
              " 0.48312109538620746,\n",
              " 0.6869963366040872,\n",
              " 0.6007230404089505,\n",
              " 0.42474770533984096,\n",
              " 0.5377478395195895,\n",
              " 0.5279890687233489,\n",
              " 0.5572419093861354,\n",
              " 0.42052043365896996,\n",
              " 0.6180176313136561,\n",
              " 0.4843090681636577,\n",
              " 0.6734567381836428,\n",
              " 0.6129360551760736,\n",
              " 0.6360539609029221,\n",
              " 0.42589405973007666,\n",
              " 0.5614056627100512,\n",
              " 0.5770807248138461,\n",
              " 0.6712457445301976]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    }
  ]
}